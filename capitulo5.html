<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Capítulo 5: Funciones de Activación Tanh y ReLU</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            background: linear-gradient(135deg, #6a11cb, #2575fc);
            color: #f0f2f0;
            line-height: 1.6;
            padding: 20px;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 16px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.3);
            padding: 40px;
            color: #333;
        }
        h1, h2, h3 {
            font-family: 'Roboto', sans-serif;
            font-weight: 700;
            color: #ffffff;
            text-align: center;
            margin-top: 0;
        }
        h1 {
            font-size: 2.5em;
            border-bottom: 2px solid #92e0ff;
            padding-bottom: 10px;
        }
        p {
            font-size: 1.1em;
            line-height: 1.8;
            text-align: justify;
            color: #f0f2f0;
        }
        .modelo-container {
            border: 1px solid #ddd;
            border-radius: 8px;
            overflow: hidden;
            margin: 30px 0;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        iframe {
            width: 100%;
            height: 600px;
            border: none;
            display: block;
        }
        .nav-links {
            text-align: center;
            margin-top: 40px;
        }
        .nav-link {
            text-decoration: none;
            color: #92e0ff;
            font-weight: bold;
            font-size: 1.1em;
            padding: 10px 20px;
            border: 1px solid #92e0ff;
            border-radius: 5px;
            transition: all 0.3s ease;
        }
        .nav-link:hover {
            background-color: #92e0ff;
            color: #2c3e50;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Funciones de Activación: Tanh y ReLU</h1>
        <p>
            Mientras que la función Sigmoide fue crucial en los inicios, las funciones Tanh y ReLU son el estándar en las redes neuronales modernas.
        </p>
        <p>
            El modelo 3D que ves a continuación te permite comparar estas dos funciones. **Tanh** comprime la salida de una neurona a un rango de **-1 a 1**, mientras que **ReLU** es más sencilla, activándose solo para valores mayores que cero. Usa el menú desplegable para cambiar entre ellas.
        </p>
        <div class="modelo-container">
            <iframe src="funciones_tanh_relu_3d.html"></iframe>
        </div>
        <p>
            **ReLU** ha demostrado ser más eficiente computacionalmente, lo que ha acelerado el entrenamiento de redes neuronales profundas.
        </p>
        <div class="nav-links">
            <a href="index.html" class="nav-link">Volver al Inicio</a>
        </div>
    </div>
</body>
</html>